{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Prepared and Maintained by [justinjohn-03](https://github.com/justinjohn0306/)**"
      ],
      "metadata": {
        "id": "maqWuu8w3dfc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "45IpwJuQv19C",
        "outputId": "f972f3b1-3e4a-44d2-98ec-8f664299db37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<font color=\"red\">Now we are set up and ready to proceed!</font>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown ### **ðŸš€ Clone Repository and Install Requirements**\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "display(HTML('<font color=\"red\">Cloning video-retalking repository:</font>'))\n",
        "!git clone https://github.com/justinjohn0306/video-retalking\n",
        "%cd video-retalking\n",
        "clear_output()\n",
        "\n",
        "display(HTML('<font color=\"red\">Uninstalling existing gdown and reinstalling from source to avoid Google Drive download quota issues:</font>'))\n",
        "!pip uninstall gdown -y\n",
        "!pip install git+https://github.com/wkentaro/gdown.git\n",
        "clear_output()\n",
        "\n",
        "display(HTML('<font color=\"red\">Installing other project requirements:</font>'))\n",
        "!pip install -r requirements_colab.txt\n",
        "clear_output()\n",
        "\n",
        "display(HTML('<font color=\"red\">Now we are set up and ready to proceed!</font>'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### **ðŸ“¥ Download the Pretrained Models**\n",
        "#@markdown The following code will download and unzip the pretrained models for this project:\n",
        "from IPython.display import clear_output, display, HTML\n",
        "%cd /content/video-retalking\n",
        "\n",
        "import gdown\n",
        "\n",
        "gdown.download(\"https://drive.google.com/uc?id=1Qtg-GVUKZ7aXtz-4O9Mm4ncXjEYRB8-p\", \"/content/checkpoints.zip\", quiet=False)\n",
        "!unzip -o /content/checkpoints.zip -d /content/video-retalking/\n",
        "!rm /content/checkpoints.zip\n",
        "clear_output()\n",
        "\n",
        "display(HTML('<font color=\"red\">Now you are ready to run the inference!</font>'))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "MpsyVNqMw6RD",
        "outputId": "a64bc28a-b628-4740-f392-9e415fa34763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<font color=\"red\">Now you are ready to run the inference!</font>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from IPython.display import clear_output, display, HTML\n",
        "\n",
        "#@markdown ### **ðŸš€ Set up Files for Inference**\n",
        "\n",
        "#@markdown Enter the path to your face video file:\n",
        "face_video = '/content/input_video.mp4'  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the path to your audio input file:\n",
        "audio_input = '/content/input_audio.wav'  #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the path where you want the output file to be saved:\n",
        "output_file = 'results/1_1.mp4'  #@param {type:\"string\"}\n",
        "\n",
        "assert os.path.exists(face_video), f\"Face video file not found: {face_video}\"\n",
        "assert os.path.exists(audio_input), f\"Audio input file not found: {audio_input}\"\n",
        "\n",
        "#@markdown Once your files are set, you can run the inference:\n",
        "display(HTML('<font color=\"red\">Running the inference...</font>'))\n",
        "!python3 inference.py --face $face_video --audio $audio_input --outfile $output_file\n"
      ],
      "metadata": {
        "id": "dGElchpX1p_i",
        "outputId": "a6abd2b1-ec76-47df-c265-e52045f393bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<font color=\"red\">Running the inference...</font>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
            "  warnings.warn(\n",
            "[Info] Using cuda for inference.\n",
            "[Step 0] Number of frames available for inference: 361\n",
            "[Step 1] Landmarks Extraction in Video.\n",
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth\" to /root/.cache/torch/hub/checkpoints/s3fd-619a316812.pth\n",
            "100% 85.7M/85.7M [00:04<00:00, 21.0MB/s]\n",
            "Downloading: \"https://www.adrianbulat.com/downloads/python-fan/2DFAN4-cd938726ad.zip\" to /root/.cache/torch/hub/checkpoints/2DFAN4-cd938726ad.zip\n",
            "100% 91.9M/91.9M [00:05<00:00, 17.7MB/s]\n",
            "landmark Det:: 100% 361/361 [00:36<00:00,  9.90it/s]\n",
            "[Step 2] 3DMM Extraction In Video:: 100% 361/361 [00:04<00:00, 77.46it/s]\n",
            "using expression center\n",
            "Load checkpoint from: checkpoints/DNet.pt\n",
            "Load checkpoint from: checkpoints/LNet.pth\n",
            "Load checkpoint from: checkpoints/ENet.pth\n",
            "[Step 3] Stablize the expression In Video:: 100% 361/361 [00:36<00:00,  9.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown ### **ðŸŽžï¸ View the Result Video**\n",
        "#@markdown After running the inference, you can view the output video directly in your browser:\n",
        "\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(output_file,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "49cunETU07_V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
